import os
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from tqdm import tqdm
from pathlib import Path
import numpy as np
import scipy.stats
import pathlib
import copy
import time

import sys
sys.path.insert(0, os.path.join(Path(__file__).parent))
import vocab
from model import Muformer
from data import PairedDataset
from utils import Saver, EarlyStopping, Logger
from criterion import pearson_correlation_loss
from fairseq import checkpoint_utils
from polynomial import PolynomialLRDecay

def setup_for_distributed(is_master):
    """
    This function disables printing when not in master process
    """
    import builtins as __builtin__
    builtin_print = __builtin__.print

    def print(*args, **kwargs):
        force = kwargs.pop('force', False)
        if is_master or force:
            builtin_print(*args, **kwargs)

    __builtin__.print = print

def init_distributed_mode(local_rank):
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        local_rank = int(os.environ["RANK"])
        local_world_size = int(os.environ['WORLD_SIZE'])
        local_gpu = int(os.environ['LOCAL_RANK'])
    else:
        print('Error when get init distributed settings!')
        local_rank = local_rank
        local_world_size = torch.cuda.device_count()
    
    print('| distributed init (rank {}): env://'.format(local_rank), flush=True)
    torch.distributed.init_process_group(
        backend='nccl',
        init_method='env://',
        world_size=local_world_size,
        rank=local_rank
    )
    torch.cuda.set_device(local_rank)

    torch.distributed.barrier()
    setup_for_distributed(local_rank==0)
    return local_rank

class Trainer(object):
    def __init__(self,
            output_dir=None,
            train_tsv=None,
            valid_tsv=None,
            test_tsv=None,
            fasta=None, 
            ccmpred_output=None,
            split_ratio=[0.9, 0.1],
            random_seed=42,
            n_ensembles=1,
            batch_size=128,
            hidden_size=256,
            dropout=0.1, 
            save_log=False,
            num_workers=1,
            aug_ratio=-1,
            locfeat_batch_encode=False,
            pretrained_model=None,
            loss='mae',
            lr=None,
            biased_layers=None,
            label_oper=None,
            decoder_name='mask-res',
            freeze_lm=False,
            joint_mode_enabled=False,
            sym_loss_weight=0.1,
            use_mask=False,
            joint_method='weighted_sum',
            local_rank=0,
            flip_train_ratio=1,
            **unused
            ):
        
        if len(unused) > 0:
            print(f'[ INFO ] Unused: \t [{unused}]', )
        print(f'[ INFO ] Pretrained: \t [{pretrained_model}]')
        
        if 'Flip' in train_tsv:
            self.flip = True
        else:
            self.flip = False
        
        tokendict_path = os.path.join(Path(__file__).parent, "protein/")

        arg_overrides = { 
            'data': tokendict_path
        }

        models, _, task = checkpoint_utils.load_model_ensemble_and_task(pretrained_model.split(os.pathsep), 
                                                                 arg_overrides=arg_overrides)
        encoder = models[0]
        self.encoder_dim = encoder.args.encoder_embed_dim
        self.num_heads = encoder.args.encoder_attention_heads

        self.hidden_size = hidden_size
        self.dropout = dropout
        self.biased_layers = None if (biased_layers is None or len(biased_layers) == 0) else [int(i) for i in biased_layers.split(',')]

        self.msa_feat_used = (self.biased_layers is not None)

        self.freeze_lm = freeze_lm
        self.joint_mode_enabled = joint_mode_enabled
        self.joint_method = joint_method
        
        self.sym_loss_weight = sym_loss_weight
        self.use_mask = use_mask
        self.n_ensembles = n_ensembles

        self.num_gpus = torch.cuda.device_count()
        self.distributed = self.num_gpus > 1
        if self.distributed:
            self.local_rank = init_distributed_mode(local_rank)
        
        self.dataset = PairedDataset(
            train_tsv=train_tsv, 
            valid_tsv=valid_tsv,
            test_tsv=test_tsv,
            fasta=fasta, 
            ccmpred_output=ccmpred_output,
            split_ratio=split_ratio,
            random_seed=random_seed,
            label_oper=label_oper,
            aug_ratio=aug_ratio,
            task=task,
            msa_feat_used=self.msa_feat_used,
            locfeat_batch_encode=locfeat_batch_encode,
            distributed=self.distributed,
            flip_train_ratio=flip_train_ratio,)
        
        self.saver = Saver(output_dir=output_dir)
        self.logger = Logger(logfile = self.saver.save_dir / 'exp.log' if save_log else None)
        self.num_workers = num_workers
        # vocab_size = len(vocab.AMINO_ACIDS)

        # seq_len = len(self.dataset.native_sequence)

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f'[ INFO ] Train data:  \t [{train_tsv}]')
        print(f'[ INFO ] Device: \t [{self.device}]')
        if self.distributed:
            self.models = [torch.nn.parallel.DistributedDataParallel(Muformer(encoder=encoder, encoder_dim=self.encoder_dim, hidden_size=self.hidden_size, num_heads=self.num_heads, dropout=self.dropout,
            biased_layers=self.biased_layers, decoder_name=decoder_name, freeze_lm=self.freeze_lm, 
            joint_mode_enabled=self.joint_mode_enabled, joint_method=self.joint_method).to(self.device),device_ids=[self.local_rank],output_device=self.local_rank, find_unused_parameters=True) for _ in range(n_ensembles)]
        else:
            self.models = [torch.nn.parallel.DataParallel(Muformer(encoder=encoder, encoder_dim=self.encoder_dim, hidden_size=self.hidden_size, num_heads=self.num_heads, dropout=self.dropout,
            biased_layers=self.biased_layers, decoder_name=decoder_name, freeze_lm=self.freeze_lm, 
            joint_mode_enabled=self.joint_mode_enabled, joint_method=self.joint_method)).to(self.device) for _ in range(n_ensembles)]
        # self.models = [Muformer(encoder=encoder, encoder_dim=self.encoder_dim, hidden_size=self.hidden_size, num_heads=self.num_heads, dropout=self.dropout,
        #     biased_layers=self.biased_layers, decoder_name=decoder_name, freeze_lm=self.freeze_lm, 
        #     joint_mode_enabled=self.joint_mode_enabled, joint_method=self.joint_method).to(self.device) for _ in range(n_ensembles)]
        
        if loss == 'mae':
            self.criterion = F.l1_loss
        elif loss == 'mse':
            self.criterion = F.mse_loss
        elif loss == 'rank':
            self.criterion = pearson_correlation_loss
        else:
            self.criterion = F.l1_loss

        self.batch_size = batch_size
        
        self.encoder_lr = 1e-5
        self.decoder_lr = 1e-4
        self.alpha_lr = 1e-2

        if lr:
            self.decoder_lr = lr

        self.clip = 1
        # print([model.keys() for model in self.models])
        self.optimizers = [optim.Adam([
            {'params': model.module.encoder.parameters(), 'lr': self.encoder_lr},              
            {'params': model.module.decoder.parameters(), 'lr': self.decoder_lr},
            {'params': model.module.attn_map.parameters(), 'lr': self.decoder_lr},
            {'params': model.module.weight_sum.parameters(), 'lr': self.decoder_lr},  
        ]) for model in self.models] #, weight_decay=1e-6

        self._test_pack = None

    @property
    def test_pack(self):
        if self._test_pack is None:
            test_loader, test_df = self.dataset.get_dataloader(
                'test', batch_size=self.batch_size, return_df=True,num_workers=self.num_workers)
            self._test_pack = (test_loader, test_df)
        return self._test_pack

    @property
    def test_loader(self):
        return self.test_pack[0]

    @property
    def test_df(self):
        return self.test_pack[1]

    def get_bare_model(self,net):
        if isinstance(net, (nn.DataParallel, nn.parallel.DistributedDataParallel)):
            net = net.module
            return net
        else:
            return net
    def load_checkpoint(self, checkpoint_dir):
        checkpoint_dir = pathlib.Path(checkpoint_dir)
        if not checkpoint_dir.is_dir():
            raise ValueError(f'[ ERRO ] {checkpoint_dir} is not a directory')
        for i in range(len(self.models)):
            checkpoint_path = f'{checkpoint_dir}/model_{i + 1}.pt'
            if self.distributed:
                if self.local_rank == 0:
                    self.logger.info('[ INFO ] Load pretrained model from [{}]'.format(checkpoint_path))
            else:
                self.logger.info('[ INFO ] Load pretrained model from [{}]'.format(checkpoint_path))
            # print('[ INFO ] Load pretrained model from [{}]'.format(checkpoint_path))
            # if self.distributed:
            #     torch.distributed.barrier()
            pt = torch.load(checkpoint_path,map_location=torch.device('cpu'))
            model_tmp = self.get_bare_model(self.models[i])
            model_dict = model_tmp.state_dict()
            model_pretrained_dict = {k: v for k, v in pt['model_state_dict'].items() if k in model_dict}
            model_dict.update(model_pretrained_dict)
            # self.models[i].load_state_dict(model_dict)
            model_tmp.load_state_dict(model_dict)
            self.optimizers[i].load_state_dict(pt['optimizer_state_dict'])

    def load_single_pretrained_model(self, checkpoint_path, model=None, optimizer=None, is_resume=False):
        if self.distributed:
            if self.local_rank == 0:
                self.logger.info('[ INFO ] Load pretrained model from [{}]'.format(checkpoint_path))
        else:
            self.logger.info('[ INFO ] Load pretrained model from [{}]'.format(checkpoint_path))
        # print('[ INFO ] Load pretrained model from [{}]'.format(checkpoint_path))
        # if self.distributed:
        #     torch.distributed.barrier()
        pt = torch.load(checkpoint_path,map_location=torch.device('cpu'))
        model_tmp = self.get_bare_model(model)
        model_dict = model_tmp.state_dict()
        model_pretrained_dict = {k: v for k, v in pt['model_state_dict'].items() if k in model_dict}
        model_dict.update(model_pretrained_dict)
        model_tmp.load_state_dict(model_dict)
        optimizer.load_state_dict(pt['optimizer_state_dict'])
        return (model, optimizer, pt['log_info']) if is_resume else (model, optimizer)

    def load_only_single_pretrained_model(self, checkpoint_path, model=None):
        if self.distributed:
            if self.local_rank == 0:
                self.logger.info('[ INFO ] Load pretrained model from [{}]'.format(checkpoint_path))
        else:
            self.logger.info('[ INFO ] Load pretrained model from [{}]'.format(checkpoint_path))
        # print('[ INFO ] Load pretrained model from [{}]'.format(checkpoint_path))
        # if self.distributed:
        #     torch.distributed.barrier()
        pt = torch.load(checkpoint_path,map_location=torch.device('cpu'))
        model_tmp = self.get_bare_model(model)
        model_dict = model_tmp.state_dict()
        model_pretrained_dict = {k: v for k, v in pt['model_state_dict'].items() if k in model_dict}
        model_dict.update(model_pretrained_dict)
        model_tmp.load_state_dict(model_dict)
        return model
    
    def load_pretrained_model(self, checkpoint_dir, models):
        for idx,model in enumerate(models):
            model = self.load_only_single_pretrained_model(os.path.join(checkpoint_dir,'model_{}.pt'.format(idx+1)),model)
        return models
    
    def save_checkpoint(self, ckp_name=None, model_dict=None, opt_dict=None, log_info=None):
        ckp = {'model_state_dict': model_dict, 'optimizer_state_dict': opt_dict}
        ckp['log_info'] = log_info
        self.saver.save_ckp(ckp, ckp_name)
        # if self.distributed:
        #     torch.distributed.barrier()

    def train(self, epochs=1000, log_freq=100, eval_freq=50,
                patience=500, save_checkpoint=False, resume_path=None,monitoring_score='corr'):
        assert eval_freq <= log_freq
        for midx, (model, optimizer) in enumerate(zip(self.models, self.optimizers), start=1):
            if self.flip or self.n_ensembles == 1:
                (train_loader, train_df), (valid_loader, valid_df) = \
                    self.dataset.get_dataloader(
                        'train_valid', self.batch_size,
                        return_df=True, resample_train_valid=False,num_workers=self.num_workers)
            else:
                (train_loader, train_df), (valid_loader, valid_df) = \
                    self.dataset.get_dataloader(
                        'train_valid', self.batch_size,
                        return_df=True, resample_train_valid=True,num_workers=self.num_workers)
            if resume_path is not None:
                model, optimizer, log_info = self.load_single_pretrained_model(
                    '{}/model_{}.pt'.format(resume_path, midx),
                    model=model, optimizer=optimizer, is_resume=True)
                start_epoch = log_info['epoch'] + 1
                best_score = log_info['best_{}'.format(monitoring_score)]
            else:
                start_epoch = 1
                best_score = None

            best_model_state_dict = None
            print(f'[ INFO ] Choice metric:  [{monitoring_score}]')
            if monitoring_score == 'loss':
                stopper = EarlyStopping(patience=patience, eval_freq=eval_freq, best_score=best_score, higher_better=False)
            elif monitoring_score == 'corr':
                stopper = EarlyStopping(patience=patience, eval_freq=eval_freq, best_score=best_score)
            model.train()
            
            steps_per_epoch = len(train_loader)

            print(f'[ INFO ] batch size       = {self.batch_size}')
            print(f'[ INFO ] #train_batches   = {len(train_loader)}')
            print(f'[ INFO ] #valid_batches   = {len(valid_loader)}')

            scheduler = PolynomialLRDecay(optimizer, total_num_update=epochs*steps_per_epoch, end_learning_rate=1e-8, power=2.0, warmup_updates=400)

            try:
                for epoch in range(start_epoch, epochs + 1):
                    if self.distributed:
                        self.dataset.sampler_train.set_epoch(epoch)
                    scheduler.step()
                    time_start = time.time()
                    tot_loss = 0
                    for step, batch_all in tqdm(enumerate(train_loader, 1),leave=False, desc=f'M-{midx} E-{epoch}', total=len(train_loader)):
                        batch1 = batch_all[0]
                        y_real = batch1['label_af'].to(self.device)

                        if self.msa_feat_used:
                            loc_feat1 = batch1['loc_feat'].to(self.device)
                        else:
                            loc_feat1 = None
                        glob_feat1 = batch1['glob_feat'].to(self.device)
                        
                        batch2 = batch_all[1]
                        if self.msa_feat_used:
                            loc_feat2 = batch2['loc_feat'].to(self.device)
                        else:
                            loc_feat2 = None
                        glob_feat2 = batch2['glob_feat'].to(self.device)
                        optimizer.zero_grad()
                        
                        if self.use_mask:
                            glob_mask1 = batch1['glob_mask'].to(self.device)
                            glob_mask2 = batch2['glob_mask'].to(self.device)
                            output = model(glob_feat1, glob_feat2, loc_feat1, loc_feat2, x1_mask=glob_mask1, x2_mask=glob_mask2)
                        else:
                            output = model(glob_feat1, glob_feat2, loc_feat1, loc_feat2)
                            
                        output = output.view(-1)
                        loss = self.criterion(output, y_real)

                        loss.backward()
                        
                        nn.utils.clip_grad_norm_(model.parameters(), self.clip)

                        optimizer.step()
                        tot_loss += loss.item()
                        
                        # if self.distributed:
                        #     torch.distributed.barrier()
                            
                    if epoch % eval_freq == 0:
                        val_results = self.test(test_model=model, test_loader=valid_loader,
                            test_df=valid_df, mode='val')

                        model.train()
                        is_best = stopper.update(val_results['metric'][monitoring_score])

                        if is_best:
                            if self.distributed:
                                net = model.module.state_dict()
                                best_model_state_dict = copy.deepcopy(model.module.state_dict())
                            else:
                                net = model.state_dict()
                                best_model_state_dict = copy.deepcopy(model.state_dict())
                            if save_checkpoint:
                                if self.distributed:
                                    if self.local_rank == 0:
                                        self.save_checkpoint(ckp_name='model_{}.pt'.format(midx),
                                            model_dict=net,
                                            opt_dict=optimizer.state_dict(),
                                            log_info={
                                                'epoch': epoch,
                                                'best_{}'.format(monitoring_score): stopper.best_score,
                                                'val_loss':val_results['loss'],
                                                'val_results':val_results['metric']
                                            })
                                    torch.distributed.barrier()
                                else:
                                    self.save_checkpoint(ckp_name='model_{}.pt'.format(midx),
                                        model_dict=net,
                                        opt_dict=optimizer.state_dict(),
                                        log_info={
                                            'epoch': epoch,
                                            'best_{}'.format(monitoring_score): stopper.best_score,
                                            'val_loss':val_results['loss'],
                                            'val_results':val_results['metric']
                                        })

                    if epoch % log_freq == 0:
                        if (log_freq < eval_freq) or (log_freq % eval_freq != 0):
                            val_results = self.test(test_model=model, test_loader=valid_loader,
                                test_df=valid_df, mode='val')

                        model.train()
                        if self.distributed:
                            if self.local_rank == 0:
                                self.logger.info(
                                    'Model: {}/{}'.format(midx, len(self.models))
                                    + ' | epoch: {}/{}'.format(epoch, epochs)
                                    # + ' | alpha: {:.4f}'.format(model.alpha.item())
                                    + ' | train loss: {:.4f}'.format(tot_loss / step)
                                    + ' | valid loss: {:.4f}'.format(val_results['loss'])                         
                                    + ' | ' + ' | '.join(['valid {}: {:.4f}'.format(k, v) \
                                            for (k, v) in val_results['metric'].items() if k!='loss'])
                                    + ' | best valid {n}: {b:.4f}'.format(n=monitoring_score, b=stopper.best_score)
                                    + ' | {:.1f} s/epoch'.format(time.time() - time_start)
                                    )
                        else:
                            self.logger.info(
                                'Model: {}/{}'.format(midx, len(self.models))
                                + ' | epoch: {}/{}'.format(epoch, epochs)
                                # + ' | alpha: {:.4f}'.format(model.alpha.item())
                                + ' | train loss: {:.4f}'.format(tot_loss / step)
                                + ' | valid loss: {:.4f}'.format(val_results['loss'])                         
                                + ' | ' + ' | '.join(['valid {}: {:.4f}'.format(k, v) \
                                        for (k, v) in val_results['metric'].items() if k!='loss'])
                                + ' | best valid {n}: {b:.4f}'.format(n=monitoring_score, b=stopper.best_score)
                                + ' | {:.1f} s/epoch'.format(time.time() - time_start)
                                )

                    if stopper.early_stop:
                        if self.distributed:
                            if self.local_rank == 0:
                                self.logger.info('Eearly stop at epoch {}'.format(epoch))
                        else:
                            self.logger.info('Eearly stop at epoch {}'.format(epoch))
                        print('Eearly stop at epoch {}'.format(epoch))
                        break
                    
                    if self.distributed:
                        torch.distributed.barrier()
            except KeyboardInterrupt:
                if self.distributed:
                    if self.local_rank == 0:
                        self.logger.info('Exiting model training from keyboard interrupt')
                else:
                    self.logger.info('Exiting model training from keyboard interrupt')
                print('Exiting model training from keyboard interrupt')
                if self.distributed:
                    torch.distributed.barrier()
                if best_model_state_dict is not None:
                    model_tmp = self.get_bare_model(model)
                    model_tmp.load_state_dict(best_model_state_dict)

                test_results = self.test(test_model=model, model_label='model_{}'.format(midx))
                test_res_msg = 'Testing Model {}: test loss: {:.4f}, '.format(midx, test_results['loss'])
                test_res_msg += ', '.join(['Test {}: {:.6f}'.format(k, v) \
                                    for (k, v) in test_results['metric'].items()])
                if self.distributed:
                    if self.local_rank == 0:
                        self.logger.info(test_res_msg + '\n')
                else:
                    self.logger.info(test_res_msg + '\n')
                print(test_res_msg + '\n')

    def test(self, test_model=None, test_loader=None, test_df=None,
                checkpoint_dir=None, 
                calc_metric=True, calc_loss=True, mode='test'):
        if checkpoint_dir is not None:
            self.models = self.load_pretrained_model(checkpoint_dir, self.models)
        if test_loader is None and test_df is None:
            test_loader = self.test_loader
            test_df = self.test_df
        test_models = self.models if test_model is None else [test_model]
        esb_ypred = None
        esb_loss = 0

        for model in test_models:
            model.eval()
            y_pred = None
            tot_loss = 0
            y_fitness = torch.Tensor([])
            with torch.no_grad():
                for step, batch_all in tqdm(enumerate(test_loader,1), desc=mode, leave=False, total=len(test_loader)):
                    batch1 = batch_all[0]
                    y_fitness = torch.cat([y_fitness, batch1['label_af']], dim=-1)

                    if self.msa_feat_used:
                        loc_feat1 = batch1['loc_feat'].to(self.device)
                    else:
                        loc_feat1 = None

                    glob_feat1 = batch1['glob_feat'].to(self.device)
                        
                    batch2 = batch_all[1]

                    if self.msa_feat_used:
                        loc_feat2 = batch2['loc_feat'].to(self.device)
                    else:
                        loc_feat2 = None

                    glob_feat2 = batch2['glob_feat'].to(self.device)
                    
                    if self.use_mask:
                        glob_mask1 = batch1['glob_mask'].to(self.device)
                        glob_mask2 = batch2['glob_mask'].to(self.device)
                        output = model(glob_feat1, glob_feat2, loc_feat1, loc_feat2, x1_mask=glob_mask1, x2_mask=glob_mask2)
                    else:
                        output = model(glob_feat1, glob_feat2, loc_feat1, loc_feat2)
                        
                    output = output.view(-1)

                    if calc_loss:
                        y = batch1['label_af'].to(self.device)
                        loss = self.criterion(output, y)
                        tot_loss += loss.item()
                    y_pred = output if y_pred is None else torch.cat((y_pred, output), dim=0)

            y_pred = y_pred.detach().cpu() if self.device == torch.device('cuda') else y_pred.detach()
            esb_ypred = y_pred.view(-1, 1) if esb_ypred is None else torch.cat((esb_ypred, y_pred.view(-1, 1)), dim=1)
            esb_loss += tot_loss / step

        esb_ypred = esb_ypred.mean(axis=1).numpy()
        esb_loss /= len(test_models)

        if calc_metric:
            y_fitness = y_fitness.numpy()
            eval_results = scipy.stats.spearmanr(y_fitness, esb_ypred)[0]

        test_results = {}
        if mode == 'test' or mode == 'ensemble':
            results_df = test_df.copy()
            results_df = results_df.drop(columns=['sequence'])
            results_df['prediction'] = esb_ypred
            test_results['df'] = results_df
            self.saver.save_df(results_df, 'prediction.tsv')
        test_results['loss'] = esb_loss
        if calc_metric:
            test_results['metric'] = {'corr': eval_results, 'loss': esb_loss}

        return test_results

    def zeroshot(self, calc_metric=True, calc_loss=True, mode='zeroshot'):
        esb_ypred = None
        esb_loss = 0
        for midx, (model, optimizer) in enumerate(zip(self.models, self.optimizers), start=1):
            zeroshot_loader, zeroshot_df = self.dataset.get_dataloader(
                'zeroshot', batch_size=self.batch_size, return_df=True,num_workers=self.num_workers)
            
            print(f'[ INFO ] batch size       = {self.batch_size}')
            print(f'[ INFO ] #zeroshot_batches   = {len(zeroshot_loader)}') 
            
            model.eval()
            y_pred = None
            tot_loss = 0
            y_fitness = torch.Tensor([])
            with torch.no_grad():
                for step, batch_all in tqdm(enumerate(zeroshot_loader,1), desc=mode, leave=False, total=len(zeroshot_loader)):
                    batch1 = batch_all[0]
                    y_fitness = torch.cat([y_fitness, batch1['label_af']], dim=-1)

                    if self.msa_feat_used:
                        loc_feat1 = batch1['loc_feat'].to(self.device)
                    else:
                        loc_feat1 = None

                    glob_feat1 = batch1['glob_feat'].to(self.device)
                        
                    batch2 = batch_all[1]

                    if self.msa_feat_used:
                        loc_feat2 = batch2['loc_feat'].to(self.device)
                    else:
                        loc_feat2 = None

                    glob_feat2 = batch2['glob_feat'].to(self.device)
                    
                    if self.use_mask:
                        glob_mask1 = batch1['glob_mask'].to(self.device)
                        glob_mask2 = batch2['glob_mask'].to(self.device)
                        output = model(glob_feat1, glob_feat2, loc_feat1, loc_feat2, x1_mask=glob_mask1, x2_mask=glob_mask2)
                    else:
                        output = model(glob_feat1, glob_feat2, loc_feat1, loc_feat2)
                        
                    output = output.view(-1)

                    if calc_loss:
                        y = batch1['label_af'].to(self.device)
                        loss = self.criterion(output, y)
                        tot_loss += loss.item()
                    y_pred = output if y_pred is None else torch.cat((y_pred, output), dim=0)

            y_pred = y_pred.detach().cpu() if self.device == torch.device('cuda') else y_pred.detach()
            esb_ypred = y_pred.view(-1, 1) if esb_ypred is None else torch.cat((esb_ypred, y_pred.view(-1, 1)), dim=1)
            esb_loss += tot_loss / step

        esb_ypred = esb_ypred.mean(axis=1).numpy()
        esb_loss /= len(self.models)

        if calc_metric:
            y_fitness = y_fitness.numpy()
            eval_results = scipy.stats.spearmanr(y_fitness, esb_ypred)[0]

        test_results = {}
        if mode == 'test' or mode == 'ensemble':
            results_df = zeroshot_df.copy()
            results_df = results_df.drop(columns=['sequence'])
            results_df['prediction'] = esb_ypred
            test_results['df'] = results_df
            self.saver.save_df(results_df, 'prediction.tsv')
        test_results['loss'] = esb_loss
        test_results['prot_name'] = self.train_tsv.split('/')[-1][:-4]
        if calc_metric:
            test_results['metric'] = {'corr': eval_results,}

        return test_results
    
    def predict_mutation(self, mutation):
        sequences = self.dataset._mutations_to_sequences([mutation])
        return self.predict(self.dataset.native_sequence, sequences[0])

    def predict(self, native_sequence, sequence):
        self.models[0].eval()
        with torch.no_grad():
            native_seq_repr = self.dataset.encode_sequence([native_sequence, sequence])
            glob_feat1 = self.dataset.encode_sequence([sequence, native_sequence]).to(self.device)
            glob_feat2 = native_seq_repr.to(self.device)
            output = self.models[0](x1=glob_feat1, x2=glob_feat2, msa_feat1=None, msa_feat2=None)
            output = output.view(-1)[0]
            return output